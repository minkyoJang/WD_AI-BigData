{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:15.751362Z",
     "start_time": "2019-08-26T07:50:03.522353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chan 2019-08-26 \n",
      "\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n",
      "konlpy 0.5.1\n",
      "torch 1.0.1\n",
      "keras 2.2.4\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a Chan -d -p numpy,pandas,konlpy,torch,keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:15.831316Z",
     "start_time": "2019-08-26T07:50:15.761362Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import konlpy\n",
    "from utils.bp_processing import bp_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:16.106164Z",
     "start_time": "2019-08-26T07:50:15.848306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ㅜㅜ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>헐</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>제시</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이거인 듯</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 comment  label\n",
       "0                     ㅜㅜ      0\n",
       "1  ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ      0\n",
       "2                      헐      0\n",
       "3                     제시      0\n",
       "4                  이거인 듯      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human = pd.read_csv('../../data/train.csv', encoding='utf-16')[['comment', 'label']]\n",
    "human.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:16.150133Z",
     "start_time": "2019-08-26T07:50:16.114158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment\n",
       "label         \n",
       "0        39904\n",
       "1         2096"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:16.172121Z",
     "start_time": "2019-08-26T07:50:16.159131Z"
    }
   },
   "outputs": [],
   "source": [
    "human_X, human_y = human[['comment']], human.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:20.177832Z",
     "start_time": "2019-08-26T07:50:16.178117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ자낳괴</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ㅇ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>실토하심ㅋㅋㅋㅋㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>돌려 돌려 돌림판~</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             comment  label\n",
       "0   ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ자낳괴      1\n",
       "1                  ㅇ      0\n",
       "2  ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ      0\n",
       "3        실토하심ㅋㅋㅋㅋㅋㅋㅋ      0\n",
       "4         돌려 돌려 돌림판~      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = pd.read_csv('../../data/auto_labeled.csv', encoding='utf-16', sep='\\t')\n",
    "auto.dropna(inplace=True)\n",
    "auto.label = auto.label.astype('int')\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:20.658559Z",
     "start_time": "2019-08-26T07:50:20.182829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1938856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment\n",
       "label         \n",
       "0      1938856\n",
       "1        61137"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:22.256646Z",
     "start_time": "2019-08-26T07:50:20.666552Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import torch.utils.data as data_utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:23.899703Z",
     "start_time": "2019-08-26T07:50:22.263639Z"
    }
   },
   "outputs": [],
   "source": [
    "auto_X, auto_y = RandomUnderSampler().fit_sample(auto[['comment']], auto.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:50:23.914696Z",
     "start_time": "2019-08-26T07:50:23.903702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61137, 122274)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_y.sum(), len(auto_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:55:01.831478Z",
     "start_time": "2019-08-26T07:55:01.814506Z"
    }
   },
   "outputs": [],
   "source": [
    "new_X = np.concatenate([human_X.values, auto_X])\n",
    "new_y = np.concatenate([human_y.values, auto_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:55:09.485104Z",
     "start_time": "2019-08-26T07:55:09.478106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((164274, 1), (164274,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X.shape, new_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:55:29.265797Z",
     "start_time": "2019-08-26T07:55:29.256798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3849239684916664"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.sum() / len(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:55:51.995798Z",
     "start_time": "2019-08-26T07:55:51.939830Z"
    }
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_X,new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:56:18.985415Z",
     "start_time": "2019-08-26T07:56:18.874430Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = [_[0] for _ in X_train]\n",
    "X_test = [_[0] for _ in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:56:22.136604Z",
     "start_time": "2019-08-26T07:56:19.207242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 123205/123205 [00:00<00:00, 560345.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 41069/41069 [00:00<00:00, 570738.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# bpe tokenizing\n",
    "tokenized_train = bp_tokenize(X_train)\n",
    "tokenized_test = bp_tokenize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:56:23.616720Z",
     "start_time": "2019-08-26T07:56:23.607724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((123205,), (41069,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train.shape, tokenized_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:56:31.750116Z",
     "start_time": "2019-08-26T07:56:29.550326Z"
    }
   },
   "outputs": [],
   "source": [
    "# padding\n",
    "MAX_LEN = 50\n",
    "x_padded_train = pad_sequences(tokenized_train, maxlen=MAX_LEN)\n",
    "x_padded_test = pad_sequences(tokenized_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:56:31.764060Z",
     "start_time": "2019-08-26T07:56:31.754065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((123205, 50), (41069, 50))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_padded_train.shape, x_padded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:56:35.947669Z",
     "start_time": "2019-08-26T07:56:35.803749Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = data_utils.TensorDataset(torch.from_numpy(x_padded_train).type(torch.LongTensor),torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
    "BATCH_SIZE = 32\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True)\n",
    "# return train_loader,x_test_pad,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:56:37.278905Z",
     "start_time": "2019-08-26T07:56:37.233935Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch,keras\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    " \n",
    "EMB_DIM = 10\n",
    "VOCAB_SIZE = 260\n",
    "class StructuredSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n",
    "    and without pruning. Slight modifications have been done for speedup\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, batch_size, lstm_hid_dim, d_a, r, max_len, emb_dim=EMB_DIM, vocab_size=VOCAB_SIZE):\n",
    "        \"\"\"\n",
    "        Initializes parameters suggested in paper\n",
    " \n",
    "        Args:\n",
    "            batch_size  : {int} batch_size used for training\n",
    "            lstm_hid_dim: {int} hidden dimension for lstm\n",
    "            d_a         : {int} hidden dimension for the dense layer\n",
    "            r           : {int} attention-hops or attention heads\n",
    "            max_len     : {int} number of lstm timesteps\n",
    "            emb_dim     : {int} embeddings dimension\n",
    "            vocab_size  : {int} size of the vocabulary\n",
    " \n",
    "        Returns:\n",
    "            self\n",
    " \n",
    "        Raises:\n",
    "            Exception\n",
    "        \"\"\"\n",
    "        super(StructuredSelfAttention,self).__init__()\n",
    "       \n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, lstm_hid_dim, 1, batch_first=True)\n",
    "        self.linear_first = torch.nn.Linear(lstm_hid_dim, d_a)\n",
    "        self.linear_first.bias.data.fill_(0)\n",
    "        self.linear_second = torch.nn.Linear(d_a, r)\n",
    "        self.linear_second.bias.data.fill_(0)\n",
    "        self.n_classes = 1\n",
    "        self.linear_final = torch.nn.Linear(lstm_hid_dim, self.n_classes)\n",
    "        self.batch_size = batch_size       \n",
    "        self.max_len = max_len\n",
    "        self.lstm_hid_dim = lstm_hid_dim\n",
    "        self.hidden_state = self.init_hidden()\n",
    "        self.r = r\n",
    "        \n",
    "    def softmax(self,input, axis=1):\n",
    "        \"\"\"\n",
    "        Softmax applied to axis=n\n",
    " \n",
    "        Args:\n",
    "           input: {Tensor,Variable} input on which softmax is to be applied\n",
    "           axis : {int} axis on which softmax is to be applied\n",
    " \n",
    "        Returns:\n",
    "            softmaxed tensors\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    " \n",
    "        input_size = input.size()\n",
    "        trans_input = input.transpose(axis, len(input_size)-1)\n",
    "        trans_size = trans_input.size()\n",
    "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "        soft_max_2d = F.softmax(input_2d)\n",
    "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "        return soft_max_nd.transpose(axis, len(input_size)-1)\n",
    "       \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n",
    "       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        embeddings = self.embeddings(x)\n",
    "        outputs, self.hidden_state = self.lstm(embeddings.view(self.batch_size,self.max_len,-1),self.hidden_state)       \n",
    "        x = F.tanh(self.linear_first(outputs))       \n",
    "        x = self.linear_second(x)       \n",
    "        x = self.softmax(x,1)       \n",
    "        attention = x.transpose(1,2)       \n",
    "        sentence_embeddings = attention@outputs       \n",
    "        avg_sentence_embeddings = torch.sum(sentence_embeddings,1)/self.r\n",
    "        \n",
    "        output = F.sigmoid(self.linear_final(avg_sentence_embeddings))\n",
    "        return output,attention\n",
    "\n",
    "       \n",
    "    #Regularization\n",
    "    def l2_matrix_norm(self,m):\n",
    "        \"\"\"\n",
    "        Frobenius norm calculation\n",
    " \n",
    "        Args:\n",
    "           m: {Variable} ||AAT - I||\n",
    " \n",
    "        Returns:\n",
    "            regularized value\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    "        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:56:38.399266Z",
     "start_time": "2019-08-26T07:56:38.349294Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def train(attention_model, train_loader, criterion, optimizer, epochs = 5, use_regularization = False , C=0, clip=False):\n",
    "    \"\"\"\n",
    "        Training code\n",
    " \n",
    "        Args:\n",
    "            attention_model : {object} model\n",
    "            train_loader    : {DataLoader} training data loaded into a dataloader\n",
    "            optimizer       :  optimizer\n",
    "            criterion       :  loss function. Must be BCELoss for binary_classification and NLLLoss for multiclass\n",
    "            epochs          : {int} number of epochs\n",
    "            use_regularizer : {bool} use penalization or not\n",
    "            C               : {int} penalization coeff\n",
    "            clip            : {bool} use gradient clipping or not\n",
    "       \n",
    "        Returns:\n",
    "            accuracy and losses of the model\n",
    " \n",
    "      \n",
    "        \"\"\"\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    for i in range(epochs):\n",
    "        print(\"Running EPOCH\",i+1)\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        correct = 0\n",
    "       \n",
    "        for batch_idx,train in enumerate(train_loader):\n",
    " \n",
    "            attention_model.hidden_state = attention_model.init_hidden()\n",
    "            x,y = Variable(train[0]),Variable(train[1])\n",
    "            y_pred,att = attention_model(x)\n",
    "           \n",
    "            #penalization AAT - I\n",
    "            if use_regularization:\n",
    "                attT = att.transpose(1,2)\n",
    "                identity = torch.eye(att.size(1))\n",
    "                identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,att.size(1),att.size(1)))\n",
    "                penal = attention_model.l2_matrix_norm(att@attT - identity)\n",
    "\n",
    "            #binary classification\n",
    "            #Adding a very small value to prevent BCELoss from outputting NaN's\n",
    "            correct+=torch.eq(torch.round(y_pred.type(torch.DoubleTensor).squeeze(1)),y).data.sum()\n",
    "            if use_regularization:\n",
    "                try:\n",
    "                    loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1)+1e-8,y) + C * penal/train_loader.batch_size\n",
    "\n",
    "                except RuntimeError:\n",
    "                    raise Exception(\"BCELoss gets nan values on regularization. Either remove regularization or add very small values\")\n",
    "            else:\n",
    "                loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1),y)\n",
    "               \n",
    " \n",
    "            total_loss+=loss.data\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "           \n",
    "            #gradient clipping\n",
    "            if clip:\n",
    "                torch.nn.utils.clip_grad_norm(attention_model.parameters(),0.5)\n",
    "            optimizer.step()\n",
    "            n_batches+=1\n",
    "           \n",
    "        print(\"avg_loss is\",total_loss/n_batches)\n",
    "        print(\"Accuracy of the model\",int(correct)/(n_batches*train_loader.batch_size))\n",
    "        losses.append(total_loss/n_batches)\n",
    "        accuracy.append(int(correct)/(n_batches*train_loader.batch_size))\n",
    "        \n",
    "        \n",
    "    return losses, accuracy\n",
    " \n",
    "def evaluate(attention_model, x_test, y_test):\n",
    "    \"\"\"\n",
    "        cv results\n",
    " \n",
    "        Args:\n",
    "            attention_model : {object} model\n",
    "            x_test          : {nplist} x_test\n",
    "            y_test          : {nplist} y_test\n",
    "       \n",
    "        Returns:\n",
    "            cv-accuracy\n",
    " \n",
    "      \n",
    "    \"\"\"\n",
    "   \n",
    "    attention_model.batch_size = x_test.shape[0]\n",
    "    attention_model.hidden_state = attention_model.init_hidden()\n",
    "    x_test_var = Variable(torch.from_numpy(x_test).type(torch.LongTensor))\n",
    "    y_test_pred,_ = attention_model(x_test_var)\n",
    "    \n",
    "    y_preds = torch.round(y_test_pred.type(torch.DoubleTensor).squeeze(1))\n",
    "    y_test_var = Variable(torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
    "    \n",
    "    print(classification_report(y_test, y_preds.detach().numpy()))\n",
    "    return int(torch.eq(y_preds,y_test_var).data.sum())/x_test_var.size(0)\n",
    " \n",
    "def get_activation_wts(attention_model,x):\n",
    "    \"\"\"\n",
    "        Get r attention heads\n",
    " \n",
    "        Args:\n",
    "            attention_model : {object} model\n",
    "            x               : {torch.Variable} input whose weights we want\n",
    "       \n",
    "        Returns:\n",
    "            r different attention weights\n",
    " \n",
    "      \n",
    "    \"\"\"\n",
    "    attention_model.batch_size = x.size(0)\n",
    "    attention_model.hidden_state = attention_model.init_hidden()\n",
    "    _,wts = attention_model(x)\n",
    "    return wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:57:30.898249Z",
     "start_time": "2019-08-26T07:57:30.888256Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_model = StructuredSelfAttention(batch_size=train_loader.batch_size,\n",
    "                                          lstm_hid_dim=10,\n",
    "                                          d_a=100,\n",
    "                                          r=1,\n",
    "                                          vocab_size=VOCAB_SIZE, \n",
    "                                          max_len=MAX_LEN,\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T08:19:27.478502Z",
     "start_time": "2019-08-26T07:57:31.841713Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss is tensor(0.4316, dtype=torch.float64)\n",
      "Accuracy of the model 0.7960146103896104\n",
      "Running EPOCH 2\n",
      "avg_loss is tensor(0.2894, dtype=torch.float64)\n",
      "Accuracy of the model 0.889172077922078\n",
      "Running EPOCH 3\n",
      "avg_loss is tensor(0.2584, dtype=torch.float64)\n",
      "Accuracy of the model 0.9025649350649351\n",
      "Running EPOCH 4\n",
      "avg_loss is tensor(0.2403, dtype=torch.float64)\n",
      "Accuracy of the model 0.9108603896103896\n",
      "Running EPOCH 5\n",
      "avg_loss is tensor(0.2273, dtype=torch.float64)\n",
      "Accuracy of the model 0.9172077922077922\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(attention_model.parameters())\n",
    "\n",
    "losses, accuracy = train(attention_model, train_loader, loss, optimizer, epochs=5,\n",
    "                         use_regularization=False, C=0.03, clip=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T08:19:47.698941Z",
     "start_time": "2019-08-26T08:19:27.501491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     25115\n",
      "           1       0.92      0.85      0.88     15954\n",
      "\n",
      "    accuracy                           0.91     41069\n",
      "   macro avg       0.91      0.90      0.91     41069\n",
      "weighted avg       0.91      0.91      0.91     41069\n",
      "\n",
      "0.9131218193771458\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(attention_model, x_padded_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T08:42:04.869265Z",
     "start_time": "2019-08-26T08:42:04.855276Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_attention(wts, X_test, filename):\n",
    "    wts_add = torch.sum(wts, 1)\n",
    "    wts_add_np = wts_add.data.numpy()\n",
    "    wts_add_list = wts_add_np.tolist()\n",
    "    text = []\n",
    "    for test in X_test:\n",
    "        text.append(\" \".join(test))\n",
    "    createHTML(text, wts_add_list, filename)\n",
    "    print(\"Attention visualization created for {} samples\".format(len(X_test)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T08:42:24.217202Z",
     "start_time": "2019-08-26T08:42:06.264476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "attention_model.batch_size = np.array(x_padded_test).shape[0]\n",
    "attention_model.hidden_state = attention_model.init_hidden()\n",
    "x_test_var = Variable(torch.from_numpy(x_padded_test).type(torch.LongTensor))\n",
    "y_test_pred,_ = attention_model(x_test_var)\n",
    "y_preds = torch.round(y_test_pred.type(torch.DoubleTensor).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T08:52:59.190429Z",
     "start_time": "2019-08-26T08:52:59.181433Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.visualize_attention import chat_to_byteLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T08:57:59.849313Z",
     "start_time": "2019-08-26T08:57:54.990076Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_to_bytelengths = list(map(chat_to_byteLength, X_test))\n",
    "chat_to_bytelength_lamb = lambda x: '_'*(50-len(x)) + x\n",
    "atttxt = list(map(chat_to_bytelength_lamb, chat_to_bytelengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T08:59:02.103704Z",
     "start_time": "2019-08-26T08:59:00.606560Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\torch\\nn\\functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention visualization created for 500 samples\n"
     ]
    }
   ],
   "source": [
    "test_last_idx = -500\n",
    "wts = get_activation_wts(attention_model,\n",
    "                         Variable(torch.from_numpy(x_padded_test[y_preds.data.numpy()==1][test_last_idx:]).type(torch.LongTensor)))\n",
    "# print(torch.sum(wts,1).data.numpy().tolist())\n",
    "visualize_attention(wts, np.array(atttxt)[y_preds.data.numpy()==1][test_last_idx:], filename='bpe_attention.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T08:41:58.921665Z",
     "start_time": "2019-08-26T08:41:58.844710Z"
    }
   },
   "outputs": [],
   "source": [
    "#Credits to Lin Zhouhan(@hantek) for the complete visualization code\n",
    "import random, os, numpy, scipy\n",
    "from codecs import open\n",
    "def createHTML(texts, weights, fileName):\n",
    "    \"\"\"\n",
    "    Creates a html file with text heat.\n",
    "\tweights: attention weights for visualizing\n",
    "\ttexts: text on which attention weights are to be visualized\n",
    "    \"\"\"\n",
    "    fileName = \"visualization/\"+fileName\n",
    "    fOut = open(fileName, \"w\", encoding=\"utf-8\")\n",
    "    part1 = \"\"\"\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\">\n",
    "    <style>\n",
    "    body {\n",
    "    font-family: Sans-Serif;\n",
    "    }\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h3>\n",
    "    Heatmaps\n",
    "    </h3>\n",
    "    </body>\n",
    "    <script>\n",
    "    \"\"\"\n",
    "    part2 = \"\"\"\n",
    "    var color = \"255,0,0\";\n",
    "    var ngram_length = 9;\n",
    "    var half_ngram = 3;\n",
    "    for (var k=0; k < any_text.length; k++) {\n",
    "    var tokens = any_text[k].split(\" \");\n",
    "    var intensity = new Array(tokens.length);\n",
    "    var max_intensity = Number.MIN_SAFE_INTEGER;\n",
    "    var min_intensity = Number.MAX_SAFE_INTEGER;\n",
    "    for (var i = 0; i < intensity.length; i++) {\n",
    "    intensity[i] = 0.0;\n",
    "    for (var j = -half_ngram; j < ngram_length-half_ngram; j++) {\n",
    "    if (i+j < intensity.length && i+j > -1) {\n",
    "    intensity[i] += trigram_weights[k][i + j];\n",
    "    }\n",
    "    }\n",
    "    if (i == 0 || i == intensity.length-1) {\n",
    "    intensity[i] /= 6.0;\n",
    "    } else {\n",
    "    intensity[i] /= 9.0;\n",
    "    }\n",
    "    if (intensity[i] > max_intensity) {\n",
    "    max_intensity = intensity[i];\n",
    "    }\n",
    "    if (intensity[i] < min_intensity) {\n",
    "    min_intensity = intensity[i];\n",
    "    }\n",
    "    }\n",
    "    var denominator = max_intensity - min_intensity;\n",
    "    for (var i = 0; i < intensity.length; i++) {\n",
    "    intensity[i] = (intensity[i] - min_intensity) / denominator;\n",
    "    }\n",
    "    if (k%2 == 0) {\n",
    "    var heat_text = \"<p><br><b>Example:</b><br>\";\n",
    "    } else {\n",
    "    var heat_text = \"<b>Example:</b><br>\";\n",
    "    }\n",
    "    var space = \"\";\n",
    "    for (var i = 0; i < tokens.length; i++) {\n",
    "    heat_text += \"<span style='background-color:rgba(\" + color + \",\" + intensity[i] + \")'>\" + space + tokens[i] + \"</span>\";\n",
    "    if (space == \"\") {\n",
    "    space = \" \";\n",
    "    }\n",
    "    }\n",
    "    //heat_text += \"<p>\";\n",
    "    document.body.innerHTML += heat_text;\n",
    "    }\n",
    "    </script>\n",
    "    </html>\"\"\"\n",
    "    putQuote = lambda x: \"\\\"%s\\\"\"%x\n",
    "    textsString = \"var any_text = [%s];\\n\"%(\",\".join(map(putQuote, texts)))\n",
    "    weightsString = \"var trigram_weights = [%s];\\n\"%(\",\".join(map(str,weights)))\n",
    "    fOut.write(part1)\n",
    "    fOut.write(textsString)\n",
    "    fOut.write(weightsString)\n",
    "    fOut.write(part2)\n",
    "    fOut.close()\n",
    "  \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:09:07.146512Z",
     "start_time": "2019-08-26T09:09:06.960603Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type StructuredSelfAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(attention_model, '../model/self_attention_bp_mixed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
